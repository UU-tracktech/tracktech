[Main]
# [ENVIRONMENT VAR REPLACES THIS IF SET] mode values: tornado, opencv, deploy
mode = deploy
# port used to host Tornado display for processor
port = 9090
# location of webpage folder, currently used for storing index file for Tornado display of proccessor
html_dir_path = ./webpage
# [ENVIRONMENT VAR REPLACES THIS IF SET] available detectors: yolov5, yolor
detector = yolov5
# [ENVIRONMENT VAR REPLACES THIS IF SET] available trackers: sort, sort_oh
tracker = sort
# [ENVIRONMENT VAR REPLACES THIS IF SET] available reid: torchreid, fastreid
reid = fastreid

[Input]
# type values: webcam, images, video, hls
type = hls
# webcam id of connected webcam range from 0 to n - 1, should be 0 when one webcam is connected to the system
webcam_device_nr = 0
# directory of images used when image capture is selected
images_dir_path = ./data/annotated/test/img1
# path to video file used when video capture is used
video_file_path = ./data/videos/venice.mp4
# [ENVIRONMENT VAR REPLACES THIS IF SET] HLS url to HLS stream that should be processed by the processor
hls_url = https://tracktech.ml:50008/stream.m3u8
# [ENVIRONMENT VAR REPLACES THIS IF SET] camera id of HLS video feed that is used to sync with the interface
camera_id = test id

[Orchestrator]
# [ENVIRONMENT VAR REPLACES THIS IF SET] url of orchestrator
# use 'ws://localhost:80/processor' when orchestrator is run locally
url = ws://tracktech.ml:50010/processor

[Filter]
# path to filter containing only the object names to detect
# discarding all classified objects that have the wrong classification
targets_path = ./filter.names

[Yolov5]
# path to video to test
source_path = ./data/videos/short_venice.mp4
# path to weights file of the used neural network
weights_path = ./yolov5s.pt
img-size = 640
conf-thres = 0.25
iou-thres = 0.45
device = 0
classes
agnostic-nms = false
augment = false
update = false
project = runs/detect
name = exp
exist-ok = false

[Yolor]
# path to video to test
source_path = ./data/videos/short_venice.mp4
# path to weights file of the used neural network
weights_path = ./yolor_p6.pt
# path to config associated with used weights file
cfg_path = ./processor/pipeline/detection/yolor/cfg/yolor_p6.cfg
names_path = ./coco.names
img-size = 640
conf-thres = 0.25
iou-thres = 0.45
device = 0
classes
agnostic-nms = false
augment = false
stride = 64

# SORT config used for both SORT and SORT_OH
[SORT]
# amount of frames a tracker persists while not found by tracker
max_age = 30
# consecutive hits to output tracker
min_hits = 0
# intersection over union used: compare predicted bounding box with received detection
iou_threshold = 0.3

[TorchReid]
size = (256, 128)
# name of the used model
model_name = osnet_x1_0
# directory of weights file of the used neural network
weights_dir_path = ./processor/pipeline/reidentification/torchreid/weights
# used device type
device = cuda
# max euclidean distance between feature vectors to pass as a re-identification
threshold = 10.5

[FastReid]
size = (256, 128)
# directory of weights file of the used neural network
weights_dir_path = ./processor/pipeline/reidentification/fastreid/weights
# path to config
config_file_path = ./processor/pipeline/reidentification/fastreid_config.yml
parallel = False
# confidence threshold to pass
threshold = 0.97

[Training]
# mode values: yolov5, yolor
mode = yolor
file = /train.py

[Training_Yolov5]
data = /data/coco128.yaml
cfg = /models/yolov5s.yaml
weights = \'\'
batch-size = 4
hyp = /data/hyp.scratch.yaml

[Training_Yolor]
multi-gpu = False
data = /data/coco.yaml
cfg = /cfg/yolor_p6.cfg
img = 1280 1280
device = 0
name = yolor_p6
weights = \'\'
batch-size = 4
hyp = /data/hyp.scratch.1280.yaml
epochs = 300

[Training_Torchreid]
root = ../../../data/annotated
sources = market1501
targets = market1501
width = 256
height = 128
batch_size_train = 32
batch_size_test = 100
model = resnet50
save_dir = log/resnet50
max_epoch = 60
eval_freq = 10
print_freq = 10

[Accuracy]
detector = yolov5
tracker = sort
source_path = ./data/annotated/test/img1
det_path = ./data/annotated/test/det/configtest.json
gt_path = ./data/annotated/COCO/annotations/instances_val2017.json
image_path = ./data/annotated/COCO/images/
plots_path = ./data/plots
plots_prefix = ohno
image_count = 50
det_format = COCO
categories = person
nr_frames = 50
