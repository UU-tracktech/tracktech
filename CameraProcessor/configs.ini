[Main]
# [ENVIRONMENT VAR REPLACES THIS IF SET] mode values: tornado, opencv, deploy
mode = deploy
# Port used to host Tornado display for processor.
port = 9090
# Location of webpage folder, currently used for storing index file for Tornado display of processor.
html_dir_path = ./webpage
# [ENVIRONMENT VAR REPLACES THIS IF SET] available detectors: yolov5, yolor
detector = yolov5
# [ENVIRONMENT VAR REPLACES THIS IF SET] available trackers: sort, sort_oh
tracker = sort
# [ENVIRONMENT VAR REPLACES THIS IF SET] available reid: torchreid, fastreid
reid = fastreid

[Input]
# Type values: webcam, images, video, hls
type = hls
# Webcam id of connected webcam range from 0 to n - 1, should be 0 when one webcam is connected to the system.
webcam_device_nr = 0
# Directory of images used when image capture is selected.
images_dir_path = ./data/annotated/test/img1
# Path to video file used when video capture is used.
video_file_path = ./data/videos/venice.mp4

[Filter]
# Path to filter containing only the object names to detect.
# Discarding all classified objects that have the wrong classification.
targets_path = ./filter.names

[Yolov5]
# Path to video to test.
source_path = ./data/videos/short_venice.mp4
# Path to weights file of the used neural network.
weights_path = ./yolov5s.pt
# Inference size in pixels.
img-size = 640
# Confidence threshold, 0 <= value <= 1
conf-thres = 0.25
# Non-maximum suppression intersection over union threshold, 0 <= value <= 1
iou-thres = 0.45
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
classes
# Class-agnostic non-maximum suppression.
agnostic-nms = false
# Use augmented inference.
augment = false
# Update all models with found detections.
update = false
# Save location of results.
project = runs/detect
# Name of saved results.
name = exp
exist-ok = false

[Yolor]
# Path to video to test.
source_path = ./data/videos/short_venice.mp4
# Path to weights file of the used neural network.
weights_path = ./yolor_p6.pt
# Path to config associated with used weights file.
cfg_path = ./processor/pipeline/detection/yolor/cfg/yolor_p6.cfg
# Names of all classifications included in the model.
names_path = ./coco.names
# Inference size in pixels.
img-size = 640
# Confidence threshold, 0 <= value <= 1
conf-thres = 0.25
# Non-maximum suppression intersection over union threshold, 0 <= value <= 1
iou-thres = 0.45
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
classes
# Class-agnostic non-maximum suppression.
agnostic-nms = false
# Use augmented inference.
augment = false
# Amount of pixels the neural network moves at a time.
stride = 64

# SORT config used for both SORT and SORT_OH.
[SORT]
# Amount of frames a tracker persists while not found by tracker.
max_age = 30
# Consecutive hits to output tracker.
min_hits = 0
# Intersection over union used: compare predicted bounding box with received detection.
iou_threshold = 0.3

[TorchReid]
# Static dimensions in pixels of the cutout over which the re-identification is run.
size = (256, 128)
# Name of the used model.
model_name = osnet_x1_0
# Directory of weights file of the used neural network.
weights_dir_path = ./processor/pipeline/reidentification/torchreid/weights
# Used device type (cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan)
device = cuda
# Max euclidean distance between feature vectors to pass as a re-identification.
threshold = 10.5
distance = euclidean

[FastReid]
# Static dimensions in pixels of the cutout over which the re-identification is run.
size = (256, 128)
# Directory of weights file of the used neural network.
weights_dir_path = ./processor/pipeline/reidentification/fastreid/weights
# Path to config.
config_file_path = ./processor/pipeline/reidentification/fastreid_config.yml
# Whether to run in parallel.
parallel = False
# Confidence threshold to pass.
threshold = 0.97
distance = cosine

[Training]
# mode_det values: yolov5, yolor
mode_det = yolor
# mode_reid values: torch, fast
mode_reid = fast
file = /train.py

[Training_Yolov5]
# Dataset configuration.
data = /data/coco128.yaml
# Model configuration.
cfg = /models/yolov5s.yaml
# Weights configuration.
weights = \'\'
# Size of batch for training.
batch-size = 4
# Hyp file for training.
hyp = /data/hyp.scratch.yaml

[Training_Yolor]
# Use multiple gpus during training.
multi-gpu = False
# Dataset configuration.
data = /data/coco.yaml
# Model configuration.
cfg = /cfg/yolor_p6.cfg
# Size of images used in training.
img = 1280 1280
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
# Name of the model to be trained.
name = yolor_p6
# Weights configuration.
weights = \'\'
# Size of batch for training.
batch-size = 4
# Hyp file for training.
hyp = /data/hyp.scratch.1280.yaml
# Number of epochs to run training on.
epochs = 300

[Training_Torchreid]
# Dataset configuration.
root = ../../../data/annotated
# Dataset source.
sources = market1501
# Dataset target.
targets = market1501
# Image width.
width = 256
# Image height.
height = 128
# Batch size for training.
batch_size_train = 32
# Batch size for testing.
batch_size_test = 100
# Model used for training and testing.
model = resnet50
# Logging filepath.
save_dir = log/resnet50
# Maximum number of epochs to run.
max_epoch = 60
# Frequency of evaluation.
eval_freq = 10
# Frequency of printing.
print_freq = 10


[Training_Fastreid]
num_gpus = 1
config_file = /configs/Market1501/bagtricks_R50.yml
model_device = cuda
device_number = 0

[Accuracy]
# Detection algorithm to use for accuracy. Values: yolov5, yolor
detector = yolov5
# Tracking algorithm to use for accuracy. Values: sort, sort_oh
tracker = sort
# Plot images filename prefix.
plots_prefix = ''
# Plot images save location filepath.
plots_path = ./data/plots/
# Format for ground truth. Format values: MOT, JSON, COCO
gt_format = MOT
# Format for detected values. Format values: MOT, JSON, COCO
det_format = JSON
# Categories to calculate accuracy over. Categories values: see filter.names
categories = person
# Number of frame to calculate accuracy over.
nr_frames = 50

[COCO]
# Annotations filepath for COCO format.
annotations_path = ./data/annotated/COCO/annotations/instances_val2017.json
# Image filepath for COCO format.
image_path = ./data/annotated/COCO/images/
# Calculated plots save location filepath for COCO format.
plots_path = ./data/plots/COCO

[MOT]
# Annotations filepath for MOT format.
annotations_path = ./data/annotated/MOT20/train/MOT20-02/gt/gt.txt
# Image filepath for MOT format.
image_path = ./data/annotated/MOT20/train/MOT20-02/img1
# Calculated plots save location filepath for MOT format.
plots_path = ./data/plots/MOT

[JSON]
# Annotations filepath for JSON format.
annotations_path = ./data/tests/unittests/mottest/det/configtest.json
