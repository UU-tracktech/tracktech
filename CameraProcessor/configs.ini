[Main]
# [ENVIRONMENT VAR REPLACES THIS IF SET] mode values: tornado, opencv, deploy
mode = deploy
# Port used to host Tornado display for processor.
port = 9090
# Location of webpage folder, currently used for storing index file for Tornado display of processor.
html_dir_path = ./webpage
# [ENVIRONMENT VAR REPLACES THIS IF SET] available detectors: yolov5, yolor
detector = yolov5
# [ENVIRONMENT VAR REPLACES THIS IF SET] available trackers: sort, sort_oh
tracker = sort
# [ENVIRONMENT VAR REPLACES THIS IF SET] available reid: torchreid, fastreid
reid = fastreid

[Input]
# Type values: webcam, images, video, hls
type = hls
# Webcam id of connected webcam range from 0 to n - 1, should be 0 when one webcam is connected to the system.
webcam_device_nr = 0
# Directory of images used when image capture is selected.
images_dir_path = ./data/annotated/test/img1
# Path to video file used when video capture is used.
video_file_path = ./data/videos/venice.mp4
# [ENVIRONMENT VAR REPLACES THIS IF SET] HLS url to HLS stream that should be processed by the processor.
hls_url = https://tracktech.ml:50008/stream.m3u8
# [ENVIRONMENT VAR REPLACES THIS IF SET] camera id of HLS video feed that is used to sync with the interface.
camera_id = test id

[Orchestrator]
# [ENVIRONMENT VAR REPLACES THIS IF SET] url of orchestrator.
# Use 'ws://localhost:80/processor' when orchestrator is run locally.
url = ws://tracktech.ml:50010/processor

[Filter]
# Path to filter containing only the object names to detect.
# Discarding all classified objects that have the wrong classification.
targets_path = ./filter.names

[Yolov5]
# Path to video to test.
source_path = ./data/videos/short_venice.mp4
# Path to weights file of the used neural network.
weights_path = ./yolov5s.pt
# Inference size in pixels.
img-size = 640
# Confidence threshold, 0 <= value <= 1
conf-thres = 0.25
# Non-maximum suppression intersection over union threshold, 0 <= value <= 1
iou-thres = 0.45
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
classes
# Class-agnostic non-maximum suppression.
agnostic-nms = false
# Use augmented inference.
augment = false
# Update all models with found detections.
update = false
# Save location of results.
project = runs/detect
# Name of saved results.
name = exp
exist-ok = false

[Yolor]
# Path to video to test.
source_path = ./data/videos/short_venice.mp4
# Path to weights file of the used neural network.
weights_path = ./yolor_p6.pt
# Path to config associated with used weights file.
cfg_path = ./processor/pipeline/detection/yolor/cfg/yolor_p6.cfg
# Names of all classifications included in the model.
names_path = ./coco.names
# Inference size in pixels.
img-size = 640
# Confidence threshold, 0 <= value <= 1
conf-thres = 0.25
# Non-maximum suppression intersection over union threshold, 0 <= value <= 1
iou-thres = 0.45
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
classes
# Class-agnostic non-maximum suppression.
agnostic-nms = false
# Use augmented inference.
augment = false
# Amount of pixels the neural network moves at a time.
stride = 64

# SORT config used for both SORT and SORT_OH.
[SORT]
# Amount of frames a tracker persists while not found by tracker.
max_age = 30
# Consecutive hits to output tracker.
min_hits = 0
# Intersection over union used: compare predicted bounding box with received detection.
iou_threshold = 0.3

[TorchReid]
# Static dimensions in pixels of the cutout over which the re-identification is run.
size = (256, 128)
# Name of the used model.
model_name = osnet_x1_0
# Directory of weights file of the used neural network.
weights_dir_path = ./processor/pipeline/reidentification/torchreid/weights
# Used device type (cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla, vulkan)
device = cuda
# Max euclidean distance between feature vectors to pass as a re-identification.
threshold = 10.5

[FastReid]
# Static dimensions in pixels of the cutout over which the re-identification is run.
size = (256, 128)
# Directory of weights file of the used neural network.
weights_dir_path = ./processor/pipeline/reidentification/fastreid/weights
# Path to config.
config_file_path = ./processor/pipeline/reidentification/fastreid_config.yml
# Whether to run in parallel.
parallel = False
# Confidence threshold to pass.
threshold = 0.97

[Training]
# Mode values: yolov5, yolor
mode = yolor
# Path to training Python file inside repo.
file = /train.py

[Training_Yolov5]
# Dataset configuration.
data = /data/coco128.yaml
# Model configuration.
cfg = /models/yolov5s.yaml
# Weights configuration.
weights = \'\'
# Size of batch for training.
batch-size = 4
# Hyp file for training.
hyp = /data/hyp.scratch.yaml

[Training_Yolor]
# Use multiple gpus during training.
multi-gpu = False
# Dataset configuration.
data = /data/coco.yaml
# Model configuration.
cfg = /cfg/yolor_p6.cfg
# Size of images used in training.
img = 1280 1280
# Device index of used gpu starting from 0 or cpu, so first gpu device = 0, second gpu device = 1, cpu device = cpu
device = 0
# Name of the model to be trained.
name = yolor_p6
# Weights configuration.
weights = \'\'
# Size of batch for training.
batch-size = 4
# Hyp file for training.
hyp = /data/hyp.scratch.1280.yaml
# Number of epochs to run training on.
epochs = 300

[Training_Torchreid]
# Dataset configuration.
root = ../../../data/annotated
# Dataset source.
sources = market1501
# Dataset target.
targets = market1501
# Image width.
width = 256
# Image height.
height = 128
# Batch size for training.
batch_size_train = 32
# Batch size for testing.
batch_size_test = 100
# Model used for training and testing.
model = resnet50
# Logging filepath.
save_dir = log/resnet50
# Maximum number of epochs to run.
max_epoch = 60
# Frequency of evaluation.
eval_freq = 10
# Frequency of printing.
print_freq = 10

[Accuracy]
detector = yolov5
tracker = sort
source_path = ./data/annotated/test/img1
det_path = ./data/annotated/test/det/configtest.json
gt_path = ./data/annotated/COCO/annotations/instances_val2017.json
image_path = ./data/annotated/COCO/images/
plots_path = ./data/plots
plots_prefix = ohno
image_count = 50
det_format = COCO
categories = person
nr_frames = 50
