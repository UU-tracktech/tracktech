import os
curr_dir = os.path.dirname(os.path.abspath(__file__))
# comment out below line to enable tensorflow logging outputs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
from absl import app, flags
from absl.flags import FLAGS
from PIL import Image
import cv2
import numpy as np

import src.pipeline.tracking.tracking_obj as TrackingObject
from src.pipeline.detection.bounding_box import BoundingBox
from src.pipeline.detection.yolov4deepsort.core.config import cfg
from src.pipeline.detection.yolov4deepsort.core import utils as utils
from src.pipeline.detection.yolov4deepsort.core.yolov4 import filter_boxes
# deep sort imports
from src.pipeline.detection.yolov4deepsort.deep_sort import nn_matching, preprocessing
from src.pipeline.detection.yolov4deepsort.deep_sort.detection import Detection
from src.pipeline.detection.yolov4deepsort.deep_sort.tracker import Tracker
from src.pipeline.detection.yolov4deepsort.tools import generate_detections as gdet

# initialize deep sort
model_filename = os.path.join(curr_dir, 'model_data/mars-small128.pb')
encoder = gdet.create_box_encoder(model_filename, batch_size=1)

framework = 'tf'
weights = os.path.join(curr_dir, 'checkpoints/yolov4-416')
size = 416
tiny = False
model = 'yolov4'
output_format = 'XVID'
iou = 0.45
score = 0.50
dont_show = False
info = False
count = False
nms_max_overlap = 1.0

# read in all class names from config, we need to change this to work for our own classes
class_names = utils.read_class_names(cfg.YOLO.CLASSES)

# by default allow all classes in .names file
allowed_classes = ['person', 'car']

flags.DEFINE_string('framework', framework, '(tf, tflite, trt')
flags.DEFINE_string('weights', weights,
                    'path to weights file')
flags.DEFINE_integer('size', size, 'resize images to')
flags.DEFINE_boolean('tiny', tiny, 'yolo or yolo-tiny')
flags.DEFINE_string('model', model, 'yolov3 or yolov4')
flags.DEFINE_string('video', './data/video/test.mp4', 'path to input video or set to 0 for webcam')
flags.DEFINE_string('output', None, 'path to output video')
flags.DEFINE_string('output_format', output_format, 'codec used in VideoWriter when saving video to file')
flags.DEFINE_float('iou', iou, 'iou threshold')
flags.DEFINE_float('score', score, 'score threshold')
flags.DEFINE_boolean('dont_show', dont_show, 'dont show video output')
flags.DEFINE_boolean('info', info, 'show detailed info of tracked objects')
flags.DEFINE_boolean('count', count, 'count objects being tracked on screen')

'''Initialize and return the tracker object, which needs to be persistent throughout
function calls to detect_and_track
'''
def initialize_tracker():
    # Definition of the parameters
    max_cosine_distance = 0.4
    nn_budget = None

    # calculate cosine distance metric
    metric = nn_matching.NearestNeighborDistanceMetric("cosine", max_cosine_distance, nn_budget)
    # initialize tracker
    tracker = Tracker(metric)
    return tracker


'''Runs both the detection YOLOv4 and the DeepSORT tracker on a given Detection Object, given a persistent tracker and a loaded model

ARGUMENTS:
det_obj: A persistent detection object
trac_obj: A "persistent" tracking object
tracker: The persistent tracker from the yolov4deepsort library
loadedmodel: A function generated by calling function based on whether to use TF or TFlite
'''
def detect_and_track(det_obj, trac_obj, tracker, loadedmodel):
    #Read frame and flip colors for some reason
    frame = cv2.cvtColor(det_obj.frame, cv2.COLOR_BGR2RGB)
    image = Image.fromarray(frame)

    #Prepare image data for detection
    image_data = cv2.resize(frame, (size, size))
    image_data = image_data / 255.
    image_data = image_data[np.newaxis, ...].astype(np.float32)

    # run detections on tflite if flag is set
    if FLAGS.framework == 'tflite':
        loadedmodel.set_tensor(input_details[0]['index'], image_data)
        loadedmodel.invoke()
        pred = [loadedmodel.get_tensor(output_details[i]['index']) for i in range(len(output_details))]
        # run detections using yolov3 if flag is set
        if model == 'yolov3' and tiny == True:
            boxes, pred_conf = filter_boxes(pred[1], pred[0], score_threshold=0.25,
                                            input_shape=tf.constant([input_size, input_size]))
        else:
            boxes, pred_conf = filter_boxes(pred[0], pred[1], score_threshold=0.25,
                                            input_shape=tf.constant([input_size, input_size]))
    #or on normal tf if not set
    else:
        batch_data = tf.constant(image_data)
        pred_bbox = loadedmodel(batch_data)
        for key, value in pred_bbox.items():
            boxes = value[:, :, 0:4]
            pred_conf = value[:, :, 4:]

    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
        boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),
        scores=tf.reshape(
            pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),
        max_output_size_per_class=50,
        max_total_size=50,
        iou_threshold=iou,
        score_threshold=score
    )

    # convert data to numpy arrays and slice out unused elements
    num_objects = valid_detections.numpy()[0]
    bboxes = boxes.numpy()[0]
    bboxes = bboxes[0:int(num_objects)]
    scores = scores.numpy()[0]
    scores = scores[0:int(num_objects)]
    classes = classes.numpy()[0]
    classes = classes[0:int(num_objects)]

    # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height
    original_h, original_w, _ = frame.shape
    bboxes = utils.format_boxes(bboxes, original_h, original_w)

    # store all predictions in one parameter for simplicity when calling functions
    pred_bbox = [bboxes, scores, classes, num_objects]

    # loop through objects and use class index to get class name, allow only classes in allowed_classes list
    names = []
    deleted_indx = []
    for i in range(num_objects):
        class_indx = int(classes[i])
        class_name = class_names[class_indx]
        if class_name not in allowed_classes:
            deleted_indx.append(i)
        else:
            names.append(class_name)
    names = np.array(names)
    count = len(names)
    if FLAGS.count:
        cv2.putText(frame, "Objects being tracked: {}".format(count), (5, 35), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2,
                    (0, 255, 0), 2)
        print("Objects being tracked: {}".format(count))
    # delete detections that are not in allowed_classes
    bboxes = np.delete(bboxes, deleted_indx, axis=0)
    scores = np.delete(scores, deleted_indx, axis=0)

    # encode yolo detections and feed to tracker
    features = encoder(frame, bboxes)
    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in
                  zip(bboxes, scores, names, features)]

    # run non-maxima supression
    boxs = np.array([d.tlwh for d in detections])
    scores = np.array([d.confidence for d in detections])
    classes = np.array([d.class_name for d in detections])
    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)
    detections = [detections[i] for i in indices]

    # Call the tracker
    tracker.predict()
    tracker.update(detections)

    #First empty the detection object's bounding box list
    det_obj.bounding_boxes = []

    # update tracks
    for track in tracker.tracks:
        if not track.is_confirmed() or track.time_since_update > 1:
            continue
        bbox = track.to_tlbr()
        class_name = track.get_class()

        #Append a BoundingBox object. Notice the None value for score: tracks do not have easily extractable scores afaik
        det_obj.bounding_boxes.append(BoundingBox(track.track_id, bbox, class_name, None))

    trac_obj = TrackingObject.TrackingObj(det_obj, None)

if __name__ == '__main__':
    try:
        app.run(main)
    except SystemExit:
        pass
